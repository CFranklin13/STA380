---
title: "Exercise 2"
author: "Clarissa Franklin, Yannick Heard, & Paige Mckenzie"
date: "August 13, 2017"
output: html_document
---

#Question 1: Flights at ABIA

### First, we read in the raw file directly from github and import the ggplot2 library
```{r, echo=FALSE, message=FALSE, warning=FALSE}
ABIA <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv", header=TRUE)
library(ggplot2)
```

We can look at the different variables available to us is this data set. Remove cancelled flights. Separate into flights departing from Austin and flights arriving in Austin.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(lubridate)
names(ABIA)
ABIA$MonthName <- month(ABIA$Month, label=TRUE, abbr=FALSE)
ABIA$MonthName <-factor(ABIA$MonthName)
```

This data set contains flights into and out of Austin. Let's create a column for Weekday name to make the data more understandable, then We can separate this data into two subsets: flights arriving into Austin, and flights departing from Austin.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
departures <- ABIA[(ABIA$Origin=="AUS") & ABIA$Cancelled==0,]
arrivals <- ABIA [(ABIA$Dest == "AUS") & ABIA$Cancelled==0,]
```

We can take a look at the summary statistics to start to understand our data.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
cat("Summary statistics: flights departing from Austin\n\n")
summary(departures)

cat("\n\nSummary statistics: flights arriving in Austin\n\n")
summary(arrivals)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

qplot( departures$DepDelay, departures$ArrDelay, xlab="Departure Delay (min)", ylab="Arrival Delay (min)", main="Austin-Bergstrom Departures")

```

### Create a historgram for average arrival and departure delays

```{r, echo=FALSE, message=FALSE, warning=FALSE}
print(hist(ABIA$ArrDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
print(hist(ABIA$DepDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
```


## aggregate (group) the flight data by month and take the mean

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~Month, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~Month, data=departures, mean, na.rm=TRUE, na.action=NULL)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Month", limits=c(1,12), breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Month")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayOfWeek, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayOfWeek, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Week", limits=c(1,7))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Week", subtitle="1 (Monday) - 7 (Sunday)")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayofMonth, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayofMonth, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Month", limits=c(1,31))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Month")
```

#Question 2: Author attribution

Let's start off by reading in the training data and setting up our corpus.
```{r}
library(tm) 
library(magrittr)
library(class)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

train = "C:/Users/Paige/Documents/MSBA/Scott/STA380/data/ReutersC50/C50train"
test = "C:/Users/Paige/Documents/MSBA/Scott/STA380/data/ReutersC50/C50test"
file_list = Sys.glob(paste0(train,'/*/*.txt'))
authornames = list.dirs("C:/Users/Paige/Documents/MSBA/Scott/STA380/data/ReutersC50/C50train", full.names = FALSE)[-1]
classificationnames = rep(authornames, each=50)
authors = lapply(file_list, readerPlain) 

mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

names(authors) = mynames
my_documents = Corpus(VectorSource(authors))

## Some pre-processing/tokenization steps copied from Scott's notes
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))


## create a doc-term-matrix
DTM = DocumentTermMatrix(my_documents)
DTM # some basic summary statistics

#remove terms that appear in less than 10% of documents
DTM = removeSparseTerms(DTM, 0.90)
```

Now let's split the training data into two sets, for cross-validation, following a 80/20 split.
```{r}
X = as.matrix(DTM)
row.names(X) = file_list_train

X = X/rowSums(X)  # term-frequency weighting

# Transform dtm to matrix to data frame
mat_df <- as.data.frame(data.matrix(DTM), stringsAsfactors = FALSE)

# Bind the categories with the known author names from the training data
mat_df$category <- classificationnames

# Split data randomly
train <- sample(nrow(mat_df), ceiling(nrow(mat_df) * .80))
test <- (1:nrow(mat_df))[- train]

# Isolate classifier
classifier <- mat_df[, "category"]

# Create model data and remove "category"
modeldata <- mat_df[,!colnames(mat_df) %in% "category"]
```

1st model: K-Nearest Neighbors
```{r}
library(caret)
# Create model
knn.pred <- knn(modeldata[train, ], modeldata[test, ], classifier[train])

conf_mat <- confusionMatrix(knn.pred, classifier[test])
conf_mat$overall[1]
```
Model 2: Naive Bayes
```{r}
library(e1071)
# Create model
model <-  naiveBayes(mat_df$category ~ ., data=mat_df)
pred <- predict(model, modeldata[test, ])

cat("Accuracy: ", sum(diag(naive.pred))/length(test))

conf.mat <- table("Predictions" = pred, Actual = classifier[test])
conf.mat
```

#Question 3: Association rule mining
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Association rule mining
# Adapted from code by Matt Taddy
library(arules)  # has a big ecosystem of packages built around it

# Read in playlists from users
grocery <- read.transactions('https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', sep=',')
summary(grocery)
inspect(grocery[1:10])
```

```{r}
library(arulesViz)
rulse_1 <- apriori(grocery, parameter=list(support=0.01, confidence=0.5))
rulse_2 <- apriori(grocery, parameter=list(support=0.001, confidence=0.9))
inspectDT(rulse_1)
inspectDT(rulse_2)
```


```{r}
rules_sorted <- sort(rules, by='confidence', decreasing=TRUE)
#matrix representation
#plot(rules[1:20], method = 'matrix', control = list(reorder=TRUE))

#Interactive Scatterplot 
plotly_arules(rules)

#plot(rules, method = 'graph', interactive=TRUE, shading=NA)
subrules <- head(sort(rules, by='lift'),10) #Graph just 10 rules by 10 highest lifts 
plot(subrules, method='graph')
plot(rules, method='grouped') #Grouped Matrix shows LHS and RHS 
plot(subrules,method='paracoord', control=list(reorder=TRUE)) #Parallel Coordinates plot for 10 rules 
```

```{r}
rules_conf <- sort(rules, by='confidence', decreasing=TRUE)
inspect(head(rules_conf)) #High-confidence rules

rules_lift <- sort(rules, by='lift', decreasing=TRUE)
inspect(head(rules_lift)) #High lift rules 
```

```{r}
rules <- apriori(data=grocery, parameter=list(supp=0.001, conf=0.08), appearance = list(default = 'lhs', rhs = 'margarine'), control=list(verbose=F))
rules <- sort(rules, decreasing=TRUE, by='confidence')
inspect(rules[1:5])
```

```{r}
rules2 <- apriori(data=grocery, parameter=list(supp=0.01, conf=0.1), appearance = list(default = 'rhs', lhs = 'margarine'), control=list(verbose=F))
rules2 <- sort(rules2, by='confidence', decreasing=TRUE)
inspect(rules2)
```






Copied from Playlists:
```{r}
# Turn user into a factor
playlists_raw$user <- factor(playlists_raw$user)

# First create a list of baskets: vectors of items by consumer
# Analagous to bags of words

# apriori algorithm expects a list of baskets in a special format
# In this case, one "basket" of songs per user
# First split data into a list of artists for each user
playlists <- split(x=playlists_raw$artist, f=playlists_raw$user)

## Remove duplicates ("de-dupe")
playlists <- lapply(playlists, unique)

## Cast this variable as a special arules "transactions" class.
playtrans <- as(playlists, "transactions")

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.5 & length (# artists) <= 4
musicrules <- apriori(playtrans, 
	parameter=list(support=.01, confidence=.5, maxlen=4))
                         
# Look at the output
inspect(musicrules)

## Choose a subset
inspect(subset(musicrules, subset=lift > 5))
inspect(subset(musicrules, subset=confidence > 0.6))
inspect(subset(musicrules, subset=support > .02 & confidence > 0.6))
```
