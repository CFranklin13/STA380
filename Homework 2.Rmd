---
title: "Exercise 2"
author: "Clarissa Franklin, Yannick Heard, & Paige Mckenzie"
date: "August 13, 2017"
output: html_document
---

#Question 1: Flights at ABIA

### First, we read in the raw file directly from github and import the ggplot2 library
```{r, echo=FALSE, message=FALSE, warning=FALSE}
ABIA <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv", header=TRUE)
library(ggplot2)
```

We can look at the different variables available to us is this data set. Remove cancelled flights. Separate into flights departing from Austin and flights arriving in Austin.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(lubridate)
names(ABIA)
ABIA$MonthName <- month(ABIA$Month, label=TRUE, abbr=FALSE)
ABIA$MonthName <-factor(ABIA$MonthName)
```

This data set contains flights into and out of Austin. Let's create a column for Weekday name to make the data more understandable, then We can separate this data into two subsets: flights arriving into Austin, and flights departing from Austin.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
departures <- ABIA[(ABIA$Origin=="AUS") & ABIA$Cancelled==0,]
arrivals <- ABIA [(ABIA$Dest == "AUS") & ABIA$Cancelled==0,]
```

We can take a look at the summary statistics to start to understand our data.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
cat("Summary statistics: flights departing from Austin\n\n")
summary(departures)

cat("\n\nSummary statistics: flights arriving in Austin\n\n")
summary(arrivals)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

qplot( departures$DepDelay, departures$ArrDelay, xlab="Departure Delay (min)", ylab="Arrival Delay (min)", main="Austin-Bergstrom Departures")

```

### Create a historgram for average arrival and departure delays

```{r, echo=FALSE, message=FALSE, warning=FALSE}
print(hist(ABIA$ArrDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
print(hist(ABIA$DepDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
```


## aggregate (group) the flight data by month and take the mean

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~Month, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~Month, data=departures, mean, na.rm=TRUE, na.action=NULL)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Month", limits=c(1,12), breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Month")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayOfWeek, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayOfWeek, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Week", limits=c(1,7))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Week", subtitle="1 (Monday) - 7 (Sunday)")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayofMonth, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayofMonth, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Month", limits=c(1,31))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Month")
```

#Question 2: Author attribution

Read in data, set up corpus
```{r}
library(tm) 
library(magrittr)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

setwd("~/MSBA/Scott/STA380")
file_list = Sys.glob('data/ReutersC50/C50train/*/*.txt')
everyone = lapply(file_list, readerPlain) 

mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

names(everyone) = mynames

my_documents = Corpus(VectorSource(everyone))
```

Do some text processing
```{r}
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
```

```{r}
## create a doc-term-matrix
DTM_simon = DocumentTermMatrix(my_documents)
DTM_simon # some basic summary statistics

class(DTM_simon)  # a special kind of sparse matrix format

## You can inspect its entries...
inspect(DTM_simon[1:10,1:20])

## ...find words with greater than a min count...
findFreqTerms(DTM_simon, 50)

## ...or find words whose count correlates with a specified word.
findAssocs(DTM_simon, "market", .5) 

## Finally, drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
##	can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.  
## Probably a bit stringent here... but only 50 docs!
DTM_simon = removeSparseTerms(DTM_simon, 0.95)
DTM_simon # now ~ 1000 terms (versus ~3000 before)



#--------------------------End of Preprocessing----------------------------------#

# Now PCA on term frequencies
X = as.matrix(DTM_simon)
X = X/rowSums(X)  # term-frequency weighting

pca_simon = prcomp(X, scale=TRUE)
plot(pca_simon) 

# Look at the loadings
pca_simon$rotation[order(abs(pca_simon$rotation[,1]),decreasing=TRUE),1][1:25]
pca_simon$rotation[order(abs(pca_simon$rotation[,2]),decreasing=TRUE),2][1:25]


## Plot the first two PCs..
plot(pca_simon$x[,1:2], xlab="PCA 1 direction", ylab="PCA 2 direction", bty="n",
     type='n')
text(pca_simon$x[,1:2], labels = 1:length(simon), cex=0.7)
identify(pca_simon$x[,1:2], n=4)

# Both about "Scottish Amicable"
content(simon[[46]])
content(simon[[48]])

# Both about genetic testing
content(simon[[25]])
content(simon[[26]])

# Both about Ladbroke's merger
content(simon[[10]])
content(simon[[11]])

```

#Question 3: Association rule mining
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Association rule mining
# Adapted from code by Matt Taddy
library(arules)  # has a big ecosystem of packages built around it

# Read in playlists from users
grocery <- read.transactions('https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', sep=',')
summary(grocery)
inspect(grocery[1:10])
```

```{r}
library(arulesViz)
rulse_1 <- apriori(grocery, parameter=list(support=0.01, confidence=0.5))
rulse_2 <- apriori(grocery, parameter=list(support=0.001, confidence=0.9))
inspectDT(rulse_1)
inspectDT(rulse_2)
```


```{r}
rules_sorted <- sort(rules, by='confidence', decreasing=TRUE)
#matrix representation
#plot(rules[1:20], method = 'matrix', control = list(reorder=TRUE))

#Interactive Scatterplot 
plotly_arules(rules)

#plot(rules, method = 'graph', interactive=TRUE, shading=NA)
subrules <- head(sort(rules, by='lift'),10) #Graph just 10 rules by 10 highest lifts 
plot(subrules, method='graph')
plot(rules, method='grouped') #Grouped Matrix shows LHS and RHS 
plot(subrules,method='paracoord', control=list(reorder=TRUE)) #Parallel Coordinates plot for 10 rules 
```

```{r}
rules_conf <- sort(rules, by='confidence', decreasing=TRUE)
inspect(head(rules_conf)) #High-confidence rules

rules_lift <- sort(rules, by='lift', decreasing=TRUE)
inspect(head(rules_lift)) #High lift rules 
```

```{r}
rules <- apriori(data=grocery, parameter=list(supp=0.001, conf=0.08), appearance = list(default = 'lhs', rhs = 'margarine'), control=list(verbose=F))
rules <- sort(rules, decreasing=TRUE, by='confidence')
inspect(rules[1:5])
```

```{r}
rules2 <- apriori(data=grocery, parameter=list(supp=0.01, conf=0.1), appearance = list(default = 'rhs', lhs = 'margarine'), control=list(verbose=F))
rules2 <- sort(rules2, by='confidence', decreasing=TRUE)
inspect(rules2)
```






Copied from Playlists:
```{r}
# Turn user into a factor
playlists_raw$user <- factor(playlists_raw$user)

# First create a list of baskets: vectors of items by consumer
# Analagous to bags of words

# apriori algorithm expects a list of baskets in a special format
# In this case, one "basket" of songs per user
# First split data into a list of artists for each user
playlists <- split(x=playlists_raw$artist, f=playlists_raw$user)

## Remove duplicates ("de-dupe")
playlists <- lapply(playlists, unique)

## Cast this variable as a special arules "transactions" class.
playtrans <- as(playlists, "transactions")

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.5 & length (# artists) <= 4
musicrules <- apriori(playtrans, 
	parameter=list(support=.01, confidence=.5, maxlen=4))
                         
# Look at the output
inspect(musicrules)

## Choose a subset
inspect(subset(musicrules, subset=lift > 5))
inspect(subset(musicrules, subset=confidence > 0.6))
inspect(subset(musicrules, subset=support > .02 & confidence > 0.6))
```