---
title: "Exercise 2"
author: "Clarissa Franklin, Yannick Heard, & Paige Mckenzie"
date: "August 13, 2017"
output: html_document
---

#Flights at ABIA

### First, we read in the raw file directly from github and import the ggplot2 library
```{r, echo=FALSE, message=FALSE, warning=FALSE}
ABIA <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv", header=TRUE)
library(ggplot2)
```

We can look at the different variables available to us is this data set. Remove cancelled flights. Separate into flights departing from Austin and flights arriving in Austin.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(lubridate)
names(ABIA)
ABIA$MonthName <- month(ABIA$Month, label=TRUE, abbr=FALSE)
ABIA$MonthName <-factor(ABIA$MonthName)
```

This data set contains flights into and out of Austin. Let's create a column for Weekday name to make the data more understandable, then We can separate this data into two subsets: flights arriving into Austin, and flights departing from Austin.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
departures <- ABIA[(ABIA$Origin=="AUS") & ABIA$Cancelled==0,]
arrivals <- ABIA [(ABIA$Dest == "AUS") & ABIA$Cancelled==0,]
```

We can take a look at the summary statistics to start to understand our data.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
cat("Summary statistics: flights departing from Austin\n\n")
summary(departures)

cat("\n\nSummary statistics: flights arriving in Austin\n\n")
summary(arrivals)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

qplot( departures$DepDelay, departures$ArrDelay, xlab="Departure Delay (min)", ylab="Arrival Delay (min)", main="Austin-Bergstrom Departures")

```

### Create a historgram for average arrival and departure delays

```{r, echo=FALSE, message=FALSE, warning=FALSE}
print(hist(ABIA$ArrDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
print(hist(ABIA$DepDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
```


## aggregate (group) the flight data by month and take the mean

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~Month, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~Month, data=departures, mean, na.rm=TRUE, na.action=NULL)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Month", limits=c(1,12), breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Month")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayOfWeek, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayOfWeek, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Week", limits=c(1,7))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Week", subtitle="1 (Monday) - 7 (Sunday)")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayofMonth, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayofMonth, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Month", limits=c(1,31))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Month")
```

#Author attribution

###Stuff
```{r}
## The tm library and related plugins comprise R's most popular text-mining stack.
## See http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf
library(tm) 
library(magrittr)

## tm has many "reader" functions.  Each one has
## arguments elem, language, id
## (see ?readPlain, ?readPDF, ?readXML, etc)
## This wraps another function around readPlain to read
## plain text documents in English.
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

## Test it on Adam Smith
adam = readerPlain("data/division_of_labor.txt")
adam
content(adam)

## apply to all of Simon Cowell's articles
## (probably not THE Simon Cowell: https://twitter.com/simoncowell)
## "globbing" = expanding wild cards in filename paths
file_list = Sys.glob('data/ReutersC50/C50train/SimonCowell/*.txt')
simon = lapply(file_list, readerPlain) 

# The file names are ugly...
file_list

# Clean up the file names
# This uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

mynames
names(simon) = mynames

## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
my_documents = Corpus(VectorSource(simon))

## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus

my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

## Remove stopwords.  Always be careful with this: one man's trash is another one's treasure.
stopwords("en")
stopwords("SMART")
?stopwords
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))


## create a doc-term-matrix
DTM_simon = DocumentTermMatrix(my_documents)
DTM_simon # some basic summary statistics

class(DTM_simon)  # a special kind of sparse matrix format

## You can inspect its entries...
inspect(DTM_simon[1:10,1:20])

## ...find words with greater than a min count...
findFreqTerms(DTM_simon, 50)

## ...or find words whose count correlates with a specified word.
findAssocs(DTM_simon, "market", .5) 

## Finally, drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
##	can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.  
## Probably a bit stringent here... but only 50 docs!
DTM_simon = removeSparseTerms(DTM_simon, 0.95)
DTM_simon # now ~ 1000 terms (versus ~3000 before)



#--------------------------End of Preprocessing----------------------------------#

# Now PCA on term frequencies
X = as.matrix(DTM_simon)
X = X/rowSums(X)  # term-frequency weighting

pca_simon = prcomp(X, scale=TRUE)
plot(pca_simon) 

# Look at the loadings
pca_simon$rotation[order(abs(pca_simon$rotation[,1]),decreasing=TRUE),1][1:25]
pca_simon$rotation[order(abs(pca_simon$rotation[,2]),decreasing=TRUE),2][1:25]


## Plot the first two PCs..
plot(pca_simon$x[,1:2], xlab="PCA 1 direction", ylab="PCA 2 direction", bty="n",
     type='n')
text(pca_simon$x[,1:2], labels = 1:length(simon), cex=0.7)
identify(pca_simon$x[,1:2], n=4)

# Both about "Scottish Amicable"
content(simon[[46]])
content(simon[[48]])

# Both about genetic testing
content(simon[[25]])
content(simon[[26]])

# Both about Ladbroke's merger
content(simon[[10]])
content(simon[[11]])

```

#Practice with association rule mining