---
title: "Exercise 2"
author: "Clarissa Franklin, Yannick Heard, & Paige Mckenzie"
date: "August 13, 2017"
output: html_document
---

# Question 1: Flights at ABIA

### First, we read in the raw file directly from github and import the ggplot2 library
```{r, echo=FALSE, message=FALSE, warning=FALSE}
ABIA <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv", header=TRUE)
library(ggplot2)
```

We can look at the different variables available to us is this data set. Remove cancelled flights. Separate into flights departing from Austin and flights arriving in Austin.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(lubridate)
names(ABIA)
ABIA$MonthName <- month(ABIA$Month, label=TRUE, abbr=FALSE)
ABIA$MonthName <-factor(ABIA$MonthName)
```

This data set contains flights into and out of Austin. Let's create a column for Weekday name to make the data more understandable, then We can separate this data into two subsets: flights arriving into Austin, and flights departing from Austin.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
departures <- ABIA[(ABIA$Origin=="AUS") & ABIA$Cancelled==0,]
arrivals <- ABIA [(ABIA$Dest == "AUS") & ABIA$Cancelled==0,]
```

We can take a look at the summary statistics to start to understand our data.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
cat("Summary statistics: flights departing from Austin\n\n")
summary(departures)

cat("\n\nSummary statistics: flights arriving in Austin\n\n")
summary(arrivals)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

qplot( departures$DepDelay, departures$ArrDelay, xlab="Departure Delay (min)", ylab="Arrival Delay (min)", main="Austin-Bergstrom Departures")

```

### Create a historgram for average arrival and departure delays

```{r, echo=FALSE, message=FALSE, warning=FALSE}
print(hist(ABIA$ArrDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
print(hist(ABIA$DepDelay, plot=TRUE, xlim=c(-30,300), freq=FALSE, breaks=seq(-180,1800,15)))
```


## aggregate (group) the flight data by month and take the mean

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~Month, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~Month, data=departures, mean, na.rm=TRUE, na.action=NULL)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=Month, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=Month, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Month", limits=c(1,12), breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Month")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayOfWeek, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayOfWeek, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayOfWeek, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Week", limits=c(1,7))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Week", subtitle="1 (Monday) - 7 (Sunday)")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

arrivals_agg <- aggregate(.~DayofMonth, data=arrivals, mean, na.rm=TRUE, na.action=NULL)
#print(arrivals_agg)

departures_agg <- aggregate(.~DayofMonth, data=departures, mean, na.rm=TRUE, na.action=NULL)

#print(departures_agg)

ggplot() +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Incoming Flights", label="ArrDelay: Incoming Flights")) +
  geom_line(data=arrivals_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Incoming Flights", label="DepDelay: Incoming Flights"))+
  geom_line(data=departures_agg, aes(x=DayofMonth, y=ArrDelay, colour="Arrival Delay: Outgoing Flights", label="ArrDelay: Outgoing Flights")) +
  geom_line(data=departures_agg, aes(x=DayofMonth, y=DepDelay, colour="Departure Delay: Outgoing Flights", label="DepDelay: Outgoing Flights"))+
  scale_color_manual(values=c("cyan4", "blue", "orchid", "orangered4")) + scale_x_continuous("Day of Month", limits=c(1,31))+
  scale_y_continuous("Average Delay (min)")+labs(title="Austin-Bergstrom Delays by Day of Month")
```

# Question 2: Author attribution
Using the Reuters 50 articles, we will try to predict the authors of some unattributed articles based on word frequency patterns. To start off, we'll read in the train and test folders, and create corpora for them.
```{r}
library(dplyr)
library(tm)
library(caret)
#Get train and test data
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
setwd("~/MSBA/Scott/STA380")

file_list_train = Sys.glob('data/ReutersC50/C50train/*/*.txt')
file_list_test = Sys.glob('data/ReutersC50/C50test/*/*.txt')
all_train = lapply(file_list_train, readerPlain) 
all_test = lapply(file_list_test, readerPlain) 

# Some more concise document names via basic string manipulation
names(all_train) = file_list_train
names(all_train) = substring(names(all_train),first=26)
names(all_train) = t(data.frame(strsplit(names(all_train),'/')))[,1]

names(all_test) = file_list_test
names(all_test) = substring(names(all_test),first=25)
names(all_test) = t(data.frame(strsplit(names(all_test),'/')))[,1]

## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
corpus_train = Corpus(VectorSource(all_train))
corpus_test = Corpus(VectorSource(all_test))
```

Next, we'll filter the corpora by making all words lowercase, removing numbers and punction, and removing excess whitespace. We'll also remove the stopwords that appear in the "en" set.
```{r}
## Some pre-processing/tokenization steps to corpus_train
corpus_train = tm_map(corpus_train, content_transformer(tolower)) # make everything lowercase
corpus_train = tm_map(corpus_train, content_transformer(removeNumbers)) # remove numbers
corpus_train = tm_map(corpus_train, content_transformer(removePunctuation)) # remove punctuation
corpus_train = tm_map(corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space

corpus_train = tm_map(corpus_train, content_transformer(removeWords), stopwords("en"))

#Do it again to corpus_test
corpus_test = tm_map(corpus_test, content_transformer(tolower)) # make everything lowercase
corpus_test = tm_map(corpus_test, content_transformer(removeNumbers)) # remove numbers
corpus_test  = tm_map(corpus_test, content_transformer(removePunctuation)) # remove punctuation
corpus_test  = tm_map(corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space

corpus_test  = tm_map(corpus_test , content_transformer(removeWords), stopwords("en"))
```
Finally, our text is processed enough to make the document term matrices. We'll also weight the words using the TF-IDF, or term frequency - inverse document frequency method. We'll also remove 'sparse' terms, using the 7.5% cut off.

Most importantly, we took the intersection of words that appear in the training articles and testing articles, and will ONLY use those words. This way, words that only appear in the testing set will not throw errors, and words that appear only in the training set won't be used at all and won't waste memory or processing time.

With all this processing done, we'll format the data into X_train and X_test, ready to fit and predict our models.
```{r}
DTM_train = DocumentTermMatrix(corpus_train, control = list(weighting = weightTfIdf))
DTM_train = removeSparseTerms(DTM_train, 0.925)

DTM_test = DocumentTermMatrix(corpus_test, control = list(weighting = weightTfIdf))
DTM_test = removeSparseTerms(DTM_test, 0.925)

#convert both to dataframe
DF_train = as.data.frame(as.matrix(DTM_train))
names(DF_train) = paste(names(DF_train),'.w',sep='')
list_authors_train = factor(names(all_train))

DF_test = as.data.frame(as.matrix(DTM_test))
names(DF_test) = paste(names(DF_test),'.w',sep='')
list_authors_test = factor(names(all_test))

#take intersection of words
intersection = intersect(names(DF_train),names(DF_test))
DF_train = DF_train[,intersection]
DF_test = DF_test[,intersection]

#split into appropriate form for model fitting
X_train = DF_train
X_train$author = list_authors_train
X_test = DF_test
X_test$author = list_authors_test
```
Model 1: Naive Bayes
```{r}
library(naivebayes)
naive_bayes_model = naive_bayes(author ~ ., data = X_train)
naive_bayes_pred = data.frame(predict(naive_bayes_model, X_test))

conf_mat_nb = confusionMatrix(table(unlist(naive_bayes_pred),X_test$author))

#Print out result (number of correct/total number of predictions)
cat("Percent correct out-of-sample for Naive Bayes:", conf_mat_nb$overall[1])

sensitivity_df_nb = as.data.frame(conf_mat_nb$byClass)
as.data.frame(sensitivity_df_nb)[order(-sensitivity_df_nb$Sensitivity),1:2]
```
Naive Bayes only accurately predicts 44% of the testing articles. The table above dispays the sensitivity and specificity for each author, once again demonstrating that Naive Bayes is not particularly accurate for the majority of the authors in the testing data. This may be because Naive Bayes assumes each word's frequency is independent, when in fact word choice may be highly correlated depending on the author's topic of interest.

Model 2: Random Forest with 350 trees
```{r}
library(randomForest)
random_forest_model = randomForest(author ~ ., data = X_train,
                          distribution = 'multinomial',
                          n.trees=350)
random_forest_pred = data.frame(predict(random_forest_model,newdata = X_test))

conf_mat_rf = confusionMatrix(table(unlist(random_forest_pred),X_test$author))

#Print out result (number of correct/total number of predictions)
cat("Percent correct out-of-sample for Random Forest:", conf_mat_rf$overall[1])

sensitivity_df_rf = as.data.frame(conf_mat_rf$byClass)
as.data.frame(sensitivity_df_rf)[order(-sensitivity_df_rf$Sensitivity),1:2]

```
Clearly, the Random Forest mode is much more accurate than the Naive Bayes, though a 55% accuracy is nothing to write home about. Once again, the table above dispays the sensitivity and specificity for each author, demonstrating that Random Forest is a much better model than Naive Bayes. 

# Question 3: Association rule mining
Using arules package for association mining
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(arules)
grocery <- read.transactions('https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', sep=',')
summary(grocery)
inspect(grocery[1:10])
```
Loading in arulesViz in order to help us look at the different levels of confidence and support along with their results to determine effectiveness
```{r}
library(arulesViz)
rules <- apriori(grocery, parameter=list(support=0.01, confidence=0.5))
rules_2 <- apriori(grocery, parameter=list(support=0.001, confidence=0.9))
inspectDT(rules)
inspectDT(rules_2)
```

Plotting results and using sorted options to allow us to only plot the top 10 rules based on lift or confidence. 
```{r}
rules_sorted <- sort(rules, by='confidence', decreasing=TRUE)
plotly_arules(rules)

subrules <- head(sort(rules, by='lift'),10) #Graph 10 rules by 10 highest lifts 
plot(subrules, method='graph')
plot(rules, method='grouped') #Grouped Matrix  to show LHS and RHS 
plot(subrules,method='paracoord', control=list(reorder=TRUE)) 
#Parallel Coordinates plot for 10 rules 
```
Allows us to look at the rules with high degrees of confidence and rules with high lift values
```{r}
rules_conf <- sort(rules, by='confidence', decreasing=TRUE)
inspect(head(rules_conf)) #High-confidence rules

rules_lift <- sort(rules, by='lift', decreasing=TRUE)
inspect(head(rules_lift)) #High lift rules 
```
This allowed us to see a lot of different basket options that indicated margarine should be included in the basket. 
```{r}
rules <- apriori(data=grocery, parameter=list(supp=0.001, conf=0.08), appearance = list(default = 'lhs', rhs = 'margarine'), control=list(verbose=F))
rules <- sort(rules, decreasing=TRUE, by='confidence')
inspect(rules[1:5])
```
Using lhs as margarine we wanted to see if it provided any knowledge, but appearing in such connected area meant it didnt have any useful insights. 
```{r}
rules2 <- apriori(data=grocery, parameter=list(supp=0.01, conf=0.1), appearance = list(default = 'rhs', lhs = 'margarine'), control=list(verbose=F))
rules2 <- sort(rules2, by='confidence', decreasing=TRUE)
inspect(rules2)
```
  We tested a few different values and combinations for support and confidence, and eventually decided to use two different levels in order to look at slightly different things. We decide on this as it made sure from a confidence level that we were making sure that there was actually a degree of consistency for that rule of above 50%. With support we kept it at 0.01 so that it would predict only options that occurred slightly more frequently so as not to waste time and effort on minor occurrences. We also tested a version with a support of 0.001 so it would pick up many different options and a confidence of 0.9, allowing us to have some knowledge about options that occur less frequently, but are far more likely. These were also both selected to prevent us having far too long of a list to work with. 
  
The discovered item sets make sense as they are typically related food items, and they primarily cover groceries that are consistent commodities. When placed into a connection map it shows that margarine is the most connected grocery, and has the greatest degree of between-ness. This agrees with association analysis that was run after at varying levels of confidence and support, as margarine was the highest rhs at all levels when sorted by confidence and lift. However having margarine as the sole item in lhs, as we screened for after, does not provide much information other than showing that you should be buying other commodities in general. Other items that had a high degree of association between them were items that were clearly related to baking, and therefore when someone was purchasing one of these items they were far more likely to be purchasing other baking items. 
For the low support and high confidence interval we found pieces of info that would impact the placement of single items near each other. This includes making sure all the alchohol is in the same section as buying wine was highly indicative of also purchasing beer. Others include cereal and milk, which could be included in the commodities section discussed below. 
## Key Grocery takeaways
	The key takeaways were that simple commodities should be placed in one area as these are often spread across stores and by providing a grouping of them you can simplify the shopping experience for people only coming for simple items. This would also hopefully help them remember all the commodities that they needed and hopefully increase revenue of the store. 



